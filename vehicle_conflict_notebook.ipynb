{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Conflict Detection Notebook\n",
    "\n",
    "This notebook demonstrates loading the synthetic vehicle dataset, preparing inputs and labels, tokenizing, and setting up fine-tuning of an open-source LLM (e.g., LLaMA 2-7B) for traffic conflict detection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install necessary packages (run once)\n",
    "!pip install transformers datasets peft accelerate torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load synthetic vehicle scenario dataset\n",
    "data = pd.read_csv('data/generated_dataset.csv')\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare input-output pairs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: concatenate features into text input\n",
    "def prepare_input(row):\n",
    "    return f'Vehicle {row.vehicle_id} in lane {row.lane} moving {row.speed} km/h towards {row.destination}, distance to intersection {row.distance_to_intersection} m, direction {row.direction}'\n",
    "\n",
    "# Target label: is_conflict + decisions\n",
    "def prepare_output(row):\n",
    "    return f'Conflict: {row.is_conflict}. Decision: {row.decisions}'\n",
    "\n",
    "# Apply functions\n",
    "data['input_text'] = data.apply(prepare_input, axis=1)\n",
    "data['output_text'] = data.apply(prepare_output, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert to Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "hf_dataset = Dataset.from_pandas(data[['input_text', 'output_text']])\n",
    "hf_dataset = hf_dataset.train_test_split(test_size=0.1)\n",
    "hf_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load a free open-source LLM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Using LLaMA 2-7B (or any smaller compatible model)\n",
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['input_text'], text_target=batch['output_text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "tokenized_ds = hf_dataset.map(tokenize_fn, batched=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set up training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./llm_vehicle_model',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start training (fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate model predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "example = 'Vehicle V7657 in lane 6 moving 62 km/h towards A, distance to intersection 319 m, direction south'\n",
    "inputs = tokenizer(example, return_tensors='pt').to(model.device)\n",
    "output_tokens = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 10
}
